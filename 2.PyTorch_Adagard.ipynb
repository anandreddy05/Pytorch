{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd"
      ],
      "metadata": {
        "id": "Si5ZuXOWTG8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- requires_grad=True:\n",
        "Setting requires_grad=True for a tensor tells PyTorch to track operations on this tensor so it can compute gradients automatically.\n",
        "\n",
        "- backward():\n",
        "Calling y.backward() computes the gradient of y with respect to x. This works because PyTorch dynamically builds a computation graph during forward propagation.\n",
        "\n",
        "- Accessing Gradients:\n",
        "The gradient is stored in x.grad. In the first example, dy/dx = 2x, so the gradient when x=5 is 10. In the second example, gradients are computed using the chain rule."
      ],
      "metadata": {
        "id": "2BNDoshle7qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "UbRQzlFMTIl4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Simple gradient computation\n",
        "# Enable require_grad to True so that PyTorch tracks gradients\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "print(x)\n",
        "y = x ** 2  # A simple quadratic function\n",
        "print(y)\n",
        "y.backward()  # Computes the gradient of y with respect to x\n",
        "print(\"Gradient of y with respect to x:\", x.grad)  # Output: 10.0"
      ],
      "metadata": {
        "id": "oKvhJqH9TKHN",
        "outputId": "00f8a80b-1016-40d1-8d29-ab2ef24a518b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5., requires_grad=True)\n",
            "tensor(25., grad_fn=<PowBackward0>)\n",
            "Gradient of y with respect to x: tensor(10.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "- y = x^2 => dy/dx = 2x\n",
        "- For x = 5, dy/dx = 2 * 5 = 10"
      ],
      "metadata": {
        "id": "p30b5nIPfHp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate chain rule\n",
        "dz/dx"
      ],
      "metadata": {
        "id": "ybWqlTUtUc8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Chain rule application\n",
        "x = torch.tensor(5.0, requires_grad=True)\n",
        "print(x)\n",
        "y = x ** 2\n",
        "print(y)\n",
        "z = torch.sin(y)  # Apply sine function on y\n",
        "print(z)\n",
        "z.backward()  # Computes the gradient of z with respect to x\n",
        "print(\"Gradient of z with respect to x:\", x.grad)"
      ],
      "metadata": {
        "id": "U_pi8_ChUJ6Y",
        "outputId": "4b421b07-1e52-4ece-8caa-81263717e42d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(5., requires_grad=True)\n",
            "tensor(25., grad_fn=<PowBackward0>)\n",
            "tensor(-0.1324, grad_fn=<SinBackward0>)\n",
            "Gradient of z with respect to x: tensor(9.9120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "- y = x^2, z = sin(y)\n",
        "- dz/dx = dz/dy * dy/dx\n",
        "- dz/dy = cos(y), dy/dx = 2x\n",
        "- Substituting x = 5.0, dy/dx = 10, and y = 25.0\n",
        "- dz/dx = cos(25.0) * 10"
      ],
      "metadata": {
        "id": "vwUx-jxOfRkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you are running multiple times it runs again from where we left, so we should clear the gradiant everytime if we want to start again.\n",
        "- To avoid this and start fresh each time, we need to zero the gradients before computing them again.\n",
        "a.grad.zero_()"
      ],
      "metadata": {
        "id": "Go8syvm4f3Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor(2.0,requires_grad=True)\n",
        "a"
      ],
      "metadata": {
        "id": "aJLL8OSbUypR",
        "outputId": "90151c00-7a61-494c-dd61-8e5901af2472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = a ** 2\n",
        "b"
      ],
      "metadata": {
        "id": "iQirOhK5U6Zk",
        "outputId": "f21b554d-a682-494d-cd85-33b9d9ddb783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.backward()\n",
        "a.grad"
      ],
      "metadata": {
        "id": "i4CNB7VkWcx5",
        "outputId": "f633e48b-9d31-471c-d67c-3de101eeeed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We use this to make tensor 0\n",
        "a.grad.zero_()"
      ],
      "metadata": {
        "id": "DTw53vs6hwG7",
        "outputId": "3145529d-6064-4842-fe37-15725634ef3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to disable gradiant tracking"
      ],
      "metadata": {
        "id": "oqEKzBmnjGrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad_(False)\n",
        "# detach()\n",
        "# torch.no_grad()"
      ],
      "metadata": {
        "id": "2wnNpo_bjGHs"
      },
      "execution_count": 83,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
