{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch NN module"
      ],
      "metadata": {
        "id": "wyY3i1yWnOvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The torch.nn module in Pytorch is a core library used to build neural networks efficiently and effectively. It abstracts the complexity of creating and training by pre-built layers, loss function adn many more.\n"
      ],
      "metadata": {
        "id": "8A0vJVJHndVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Neural Network"
      ],
      "metadata": {
        "id": "UBR-7Sk0tKzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# We have to inherit using nn.Module to use this\n",
        "class MyNN(nn.Module):\n",
        "# num_features tells that how many features are used\n",
        "  def __init__(self,num_features):\n",
        "    super().__init__()\n",
        "    # nn.Linear(number of features, number of output_features)\n",
        "    self.linear = nn.Linear(num_features,1)\n",
        "    # Activation function can be used like nn.ReLU,nn.Sigmoid....\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  def forward(self,features):\n",
        "    output = self.linear(features)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "eTxBKC3YnR97"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random dataset\n",
        "features = torch.rand(10,4)\n",
        "# create an instance of our MyNN class\n",
        "model = MyNN(features.shape[1])\n",
        "# forward pass\n",
        "model(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjtNzjHKpOwg",
        "outputId": "68987205-ba0d-405c-cd7d-c362fe0a55e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4414],\n",
              "        [0.4077],\n",
              "        [0.4266],\n",
              "        [0.3584],\n",
              "        [0.4137],\n",
              "        [0.3667],\n",
              "        [0.3628],\n",
              "        [0.4265],\n",
              "        [0.3568],\n",
              "        [0.4095]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see the weights and bias used by our nn\n",
        "print(f\"Weights: {model.linear.weight}\")\n",
        "print(\"---\"*10)\n",
        "print(f\"Bias: {model.linear.bias}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BLUZZiqptcc",
        "outputId": "3c5e5149-990b-4c19-c458-90acdfad7f74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: Parameter containing:\n",
            "tensor([[ 0.2411, -0.0843,  0.1891, -0.3246]], requires_grad=True)\n",
            "------------------------------\n",
            "Bias: Parameter containing:\n",
            "tensor([-0.4688], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can visualize our nn using torchinfo\n",
        "%pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Oh8t2jTqEkt",
        "outputId": "5ec664bb-1abb-496f-d858-ba7c10b30e61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "# it takes our instance and inputs_size (features = torch.rand(10,4))\n",
        "summary(model,input_size=(10,4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I06krEHbqa4z",
        "outputId": "dafe5b65-d160-445e-9932-ac92acf0120b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MyNN                                     [10, 1]                   --\n",
              "├─Linear: 1-1                            [10, 1]                   5\n",
              "├─Sigmoid: 1-2                           [10, 1]                   --\n",
              "==========================================================================================\n",
              "Total params: 5\n",
              "Trainable params: 5\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network with Hidden Layers"
      ],
      "metadata": {
        "id": "_pxvn3I2tPgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **linear transformation** is a mathematical operation where we multiply input features by weights and add a bias. In neural networks, this is performed by fully connected (dense) layers using nn.Linear in PyTorch.\n",
        "A linear transformation in a neural network follows this equation:\n",
        "+ 𝑌=𝑋𝑊+𝑏\n",
        "Where:\n",
        "\n",
        "- X = Input features (a vector or matrix of values)\n",
        "- W = Weights (learnable parameters that adjust how inputs affect outputs)\n",
        "- b = Bias (a learnable parameter that shifts the output)\n",
        "- Y = Output after transformation\n",
        "This operation maps the input linearly to the output."
      ],
      "metadata": {
        "id": "eb3ZQPoSwFkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "#Inherit from nn.Module to define a custom neural network\n",
        "class MyNN1(nn.Module):\n",
        "# num_features tells that how many features are used\n",
        "  def __init__(self,num_features):\n",
        "    super().__init__()\n",
        "    # Input layer with 'num_features' connected to 3 neurons\n",
        "    self.linear1 = nn.Linear(num_features,3)\n",
        "    self.relu = nn.ReLU()\n",
        "    # Hidden layer (3 neurons) connected to 1 output neuron\n",
        "    self.linear2 = nn.Linear(3,1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  # Here in forward pass\n",
        "  # features(4) -> linear1 -> relu -> linear2 -> sigmoid\n",
        "  def forward(self,features):\n",
        "    output = self.linear1(features) # Linear transformation\n",
        "    output = self.relu(output)  # Apply ReLU activation\n",
        "    output = self.linear2(output) # Another linear transformation\n",
        "    output = self.sigmoid(output) # Sigmoid activation\n",
        "    return output"
      ],
      "metadata": {
        "id": "xu33EO68qso4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random dataset\n",
        "features = torch.rand(10,4)\n",
        "# create an instance of our MyNN class\n",
        "model = MyNN1(features.shape[1])\n",
        "# forward pass\n",
        "model(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW_phYyDGzKe",
        "outputId": "49ca2523-64e1-4e32-b35e-4a510c7cc220"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4748],\n",
              "        [0.4951],\n",
              "        [0.4620],\n",
              "        [0.4874],\n",
              "        [0.4594],\n",
              "        [0.4872],\n",
              "        [0.4799],\n",
              "        [0.4530],\n",
              "        [0.4815],\n",
              "        [0.4799]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(model=model,input_size=(10,4))"
      ],
      "metadata": {
        "id": "Xmu9AM7tvKGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3040a1-6f73-40b2-8f16-5467a0a2892f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "MyNN1                                    [10, 1]                   --\n",
              "├─Linear: 1-1                            [10, 3]                   15\n",
              "├─ReLU: 1-2                              [10, 3]                   --\n",
              "├─Linear: 1-3                            [10, 1]                   4\n",
              "├─Sigmoid: 1-4                           [10, 1]                   --\n",
              "==========================================================================================\n",
              "Total params: 19\n",
              "Trainable params: 19\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Container\n",
        "Without explictly defining like this in the forward function   \n",
        "\n",
        "      output = self.linear1(features) # Linear transformation\n",
        "      output = self.relu(output)  # Apply ReLU activation\n",
        "      output = self.linear2(output) # Another linear transformation\n",
        "      output = self.sigmoid(output) # Sigmoid activation\n",
        "      return output\n",
        "  We can use Sequential from pytorch\n",
        "  `torch.nn.Sequential` is a simpler way to define neural networks in PyTorch. It eliminates the need to explicitly define the `forward` method.\n"
      ],
      "metadata": {
        "id": "Ku87hSp0HJn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class MyNN2(nn.Module):\n",
        "  def __init__(self,num_features):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "    nn.Linear(num_features,3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(3,1),\n",
        "    nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self,features):\n",
        "    output = self.network(features)\n",
        "    return output"
      ],
      "metadata": {
        "id": "jwmAHJQDG7f5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  PyTorch Loss Functions & Optimizers\n",
        "\n",
        "In PyTorch, **loss functions** and **optimizers** are essential for training neural networks. This document provides an overview of the most commonly used ones.\n",
        "\n",
        "---\n",
        "\n",
        "##  Loss Functions in PyTorch\n",
        "\n",
        "Loss functions measure how well the model's predictions match the actual values. PyTorch provides many built-in loss functions under `torch.nn`.\n",
        "\n",
        "###  Common Loss Functions\n",
        "| Loss Function | Usage | Suitable for |\n",
        "|--------------|-------|--------------|\n",
        "| `nn.MSELoss()` | Mean Squared Error | Regression |\n",
        "| `nn.L1Loss()` | Mean Absolute Error (MAE) | Regression |\n",
        "| `nn.CrossEntropyLoss()` | Combines `Softmax` + `NLLLoss` | Multi-class classification |\n",
        "| `nn.BCELoss()` | Binary Cross-Entropy | Binary classification |\n",
        "| `nn.BCEWithLogitsLoss()` | More stable than `BCELoss` (includes `Sigmoid`) | Binary classification |\n",
        "| `nn.NLLLoss()` | Negative Log Likelihood Loss | Classification (log-prob outputs) |\n",
        "| `nn.HuberLoss()` | Smooth L1 loss (less sensitive to outliers) | Regression |\n",
        "\n",
        "---\n",
        "\n",
        "##  Optimizers in PyTorch\n",
        "\n",
        "Optimizers adjust model weights to minimize the loss function. PyTorch provides optimizers in `torch.optim`.\n",
        "\n",
        "### Common Optimizers\n",
        "| Optimizer | Description | Notes |\n",
        "|-----------|-------------|-------|\n",
        "| `optim.SGD` | Stochastic Gradient Descent | Simple, requires tuning learning rate |\n",
        "| `optim.Adam` | Adaptive Moment Estimation | Works well in most cases, adaptive learning rate |\n",
        "| `optim.AdamW` | Adam with weight decay | Prevents overfitting better than Adam |\n",
        "| `optim.RMSprop` | Root Mean Square Propagation | Good for RNNs |\n",
        "| `optim.Adagrad` | Adaptive Gradient Algorithm | Adapts learning rates per parameter |\n",
        "| `optim.Adadelta` | Improved version of Adagrad | Less sensitive to hyperparameters |\n",
        "\n",
        "---\n",
        "\n",
        "## Example\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Example model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 1)\n",
        ")\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.MSELoss()  # Using Mean Squared Error for regression\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer with LR = 0.01\n",
        "\n",
        "# Example forward pass\n",
        "x = torch.randn(10)  # Random input\n",
        "output = model(x)  # Forward pass\n",
        "\n",
        "# Compute loss\n",
        "target = torch.tensor([1.0])  # Example target\n",
        "loss = criterion(output, target)  \n",
        "\n",
        "# Backpropagation\n",
        "optimizer.zero_grad()  # Clear previous gradients\n",
        "loss.backward()  # Compute gradients\n",
        "optimizer.step()  # Update model parameters\n"
      ],
      "metadata": {
        "id": "Pl-PXo3GJNp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and DataLoaders"
      ],
      "metadata": {
        "id": "TjMBsjCsIw6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, **`Dataset`** and **`DataLoader`** are used to efficiently load and process data during training.\n",
        "### **1.1 Custom Dataset using `torch.utils.data.Dataset`**\n",
        "PyTorch provides `torch.utils.data.Dataset` as a base class for custom datasets.\n"
      ],
      "metadata": {
        "id": "qJYm6fVHKyzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(100,6)\n",
        "y = torch.randint(0,2,(100,))\n",
        "print(f\"{X}\")\n",
        "print(\"=\"*100)\n",
        "print(f\"{y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTTOiEqHaKrO",
        "outputId": "a6591571-3a31-4aed-b134-2b8fa850d17e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5776, 0.9000, 0.1609, 0.9306, 0.4972, 0.4618],\n",
            "        [0.7566, 0.0880, 0.1550, 0.8078, 0.1599, 0.9444],\n",
            "        [0.7484, 0.5140, 0.2987, 0.6938, 0.6796, 0.0094],\n",
            "        [0.4809, 0.3469, 0.9955, 0.3154, 0.9456, 0.5202],\n",
            "        [0.6395, 0.3494, 0.4725, 0.3510, 0.5624, 0.8934],\n",
            "        [0.3550, 0.5643, 0.2131, 0.1157, 0.8093, 0.3874],\n",
            "        [0.9285, 0.0211, 0.7063, 0.1076, 0.2905, 0.5838],\n",
            "        [0.2188, 0.3940, 0.7828, 0.0511, 0.7277, 0.7516],\n",
            "        [0.4336, 0.8794, 0.7858, 0.6426, 0.8207, 0.3101],\n",
            "        [0.0871, 0.6215, 0.3880, 0.6541, 0.0267, 0.5145],\n",
            "        [0.3413, 0.2524, 0.7783, 0.1326, 0.7127, 0.0904],\n",
            "        [0.2215, 0.5700, 0.9435, 0.6462, 0.7096, 0.0707],\n",
            "        [0.4819, 0.2569, 0.9140, 0.5903, 0.9070, 0.9486],\n",
            "        [0.7732, 0.6609, 0.6805, 0.2378, 0.9784, 0.7439],\n",
            "        [0.2996, 0.0130, 0.8797, 0.0993, 0.0937, 0.7945],\n",
            "        [0.2765, 0.9892, 0.9798, 0.6344, 0.7299, 0.9269],\n",
            "        [0.0853, 0.3762, 0.3205, 0.6500, 0.6059, 0.7222],\n",
            "        [0.6282, 0.3804, 0.3477, 0.9349, 0.6239, 0.3063],\n",
            "        [0.4035, 0.3907, 0.2688, 0.4353, 0.3443, 0.1205],\n",
            "        [0.8930, 0.8396, 0.5133, 0.2139, 0.4861, 0.2790],\n",
            "        [0.5173, 0.6958, 0.0528, 0.8538, 0.4301, 0.7168],\n",
            "        [0.9786, 0.5752, 0.8084, 0.2121, 0.6268, 0.9375],\n",
            "        [0.4397, 0.2831, 0.2467, 0.6330, 0.4300, 0.6828],\n",
            "        [0.2068, 0.8956, 0.5724, 0.4630, 0.2774, 0.7640],\n",
            "        [0.3588, 0.7087, 0.0662, 0.7169, 0.4730, 0.1353],\n",
            "        [0.9474, 0.2379, 0.9606, 0.2483, 0.5563, 0.2781],\n",
            "        [0.6690, 0.4359, 0.9203, 0.5703, 0.1306, 0.5975],\n",
            "        [0.1383, 0.2346, 0.8305, 0.9651, 0.7634, 0.5301],\n",
            "        [0.8719, 0.3343, 0.4702, 0.2665, 0.4823, 0.5450],\n",
            "        [0.5507, 0.0065, 0.8948, 0.5024, 0.9782, 0.7293],\n",
            "        [0.5332, 0.0549, 0.9872, 0.3369, 0.3187, 0.4036],\n",
            "        [0.3776, 0.9760, 0.6006, 0.9888, 0.9478, 0.6795],\n",
            "        [0.7170, 0.1829, 0.4321, 0.2553, 0.8908, 0.8530],\n",
            "        [0.4591, 0.4444, 0.3081, 0.6307, 0.4244, 0.4820],\n",
            "        [0.8319, 0.3359, 0.9627, 0.4839, 0.5072, 0.5835],\n",
            "        [0.5452, 0.2370, 0.4522, 0.9969, 0.5851, 0.3790],\n",
            "        [0.4673, 0.8703, 0.2917, 0.5955, 0.0329, 0.4547],\n",
            "        [0.6883, 0.2423, 0.5956, 0.0339, 0.3082, 0.6250],\n",
            "        [0.1880, 0.2358, 0.6195, 0.0443, 0.9145, 0.5109],\n",
            "        [0.0965, 0.8579, 0.6330, 0.0145, 0.3575, 0.7522],\n",
            "        [0.8318, 0.3148, 0.9553, 0.1036, 0.9713, 0.3876],\n",
            "        [0.3591, 0.2662, 0.0539, 0.8810, 0.8171, 0.5528],\n",
            "        [0.2823, 0.4437, 0.1454, 0.4515, 0.7684, 0.3523],\n",
            "        [0.1812, 0.2039, 0.2554, 0.0667, 0.4029, 0.6956],\n",
            "        [0.0771, 0.2302, 0.3039, 0.7645, 0.8320, 0.3998],\n",
            "        [0.2164, 0.9807, 0.7115, 0.7718, 0.3233, 0.3942],\n",
            "        [0.3980, 0.4772, 0.4537, 0.9736, 0.7467, 0.0382],\n",
            "        [0.3052, 0.7803, 0.5779, 0.2831, 0.3858, 0.4102],\n",
            "        [0.1991, 0.5905, 0.7555, 0.0317, 0.4007, 0.3050],\n",
            "        [0.0503, 0.6719, 0.0339, 0.2324, 0.8347, 0.9505],\n",
            "        [0.5255, 0.0326, 0.8293, 0.3846, 0.7348, 0.3149],\n",
            "        [0.8296, 0.1747, 0.2829, 0.9666, 0.5546, 0.1205],\n",
            "        [0.2491, 0.9403, 0.7755, 0.8972, 0.0646, 0.0385],\n",
            "        [0.4885, 0.6915, 0.4359, 0.6367, 0.6537, 0.0936],\n",
            "        [0.0883, 0.8511, 0.2962, 0.5418, 0.3947, 0.7354],\n",
            "        [0.6505, 0.5060, 0.9934, 0.5155, 0.9647, 0.8427],\n",
            "        [0.7473, 0.7798, 0.9900, 0.9377, 0.1537, 0.8897],\n",
            "        [0.9876, 0.3681, 0.8653, 0.2028, 0.4190, 0.3275],\n",
            "        [0.6355, 0.8001, 0.1561, 0.3425, 0.8003, 0.0671],\n",
            "        [0.9449, 0.2446, 0.8405, 0.6939, 0.9687, 0.9183],\n",
            "        [0.4037, 0.8304, 0.3377, 0.1274, 0.5241, 0.6993],\n",
            "        [0.6791, 0.9115, 0.3071, 0.5021, 0.3786, 0.6088],\n",
            "        [0.2138, 0.7462, 0.0601, 0.7724, 0.4858, 0.2286],\n",
            "        [0.5008, 0.8301, 0.9907, 0.9875, 0.5571, 0.8058],\n",
            "        [0.2987, 0.5454, 0.4139, 0.1221, 0.5482, 0.0716],\n",
            "        [0.3033, 0.2507, 0.6972, 0.5489, 0.7244, 0.8616],\n",
            "        [0.2255, 0.6500, 0.7560, 0.5290, 0.5114, 0.5635],\n",
            "        [0.2663, 0.8363, 0.5950, 0.7301, 0.7248, 0.9349],\n",
            "        [0.0381, 0.4741, 0.4798, 0.1577, 0.5729, 0.2967],\n",
            "        [0.2912, 0.0195, 0.0927, 0.5971, 0.7170, 0.4106],\n",
            "        [0.3919, 0.4411, 0.9767, 0.1356, 0.4588, 0.6754],\n",
            "        [0.4770, 0.6874, 0.9437, 0.8137, 0.5849, 0.1876],\n",
            "        [0.5418, 0.5577, 0.8930, 0.0229, 0.9445, 0.6840],\n",
            "        [0.6478, 0.9683, 0.8222, 0.0620, 0.5100, 0.7791],\n",
            "        [0.1718, 0.5224, 0.6840, 0.3126, 0.5118, 0.3419],\n",
            "        [0.7277, 0.2814, 0.5271, 0.9630, 0.1450, 0.5930],\n",
            "        [0.1753, 0.5629, 0.8734, 0.7246, 0.1853, 0.8089],\n",
            "        [0.8815, 0.9248, 0.5555, 0.8305, 0.9029, 0.1655],\n",
            "        [0.9687, 0.9379, 0.6730, 0.5426, 0.2151, 0.8872],\n",
            "        [0.6229, 0.9708, 0.0359, 0.5001, 0.3949, 0.3656],\n",
            "        [0.4911, 0.9111, 0.1223, 0.2204, 0.4577, 0.6061],\n",
            "        [0.3261, 0.2044, 0.5865, 0.7402, 0.3769, 0.9266],\n",
            "        [0.7667, 0.5106, 0.5484, 0.4717, 0.5271, 0.9290],\n",
            "        [0.9656, 0.2314, 0.4524, 0.0847, 0.9257, 0.0377],\n",
            "        [0.2111, 0.8961, 0.6753, 0.4878, 0.7563, 0.4630],\n",
            "        [0.2238, 0.0486, 0.8420, 0.5740, 0.3979, 0.7728],\n",
            "        [0.3595, 0.4544, 0.4244, 0.8916, 0.1588, 0.5223],\n",
            "        [0.2158, 0.5957, 0.4134, 0.5058, 0.4373, 0.9655],\n",
            "        [0.2205, 0.1346, 0.4050, 0.1876, 0.1370, 0.7164],\n",
            "        [0.1590, 0.4278, 0.9784, 0.6540, 0.1984, 0.0554],\n",
            "        [0.9809, 0.6558, 0.9436, 0.2917, 0.9373, 0.5471],\n",
            "        [0.0671, 0.1265, 0.9818, 0.8225, 0.6243, 0.4884],\n",
            "        [0.9766, 0.4381, 0.4601, 0.5632, 0.7273, 0.1392],\n",
            "        [0.6163, 0.7452, 0.9887, 0.6003, 0.1843, 0.3144],\n",
            "        [0.3048, 0.2604, 0.3896, 0.9247, 0.4683, 0.3956],\n",
            "        [0.1047, 0.2161, 0.4091, 0.5512, 0.1253, 0.8175],\n",
            "        [0.2038, 0.7420, 0.2721, 0.8693, 0.8531, 0.6995],\n",
            "        [0.6512, 0.5459, 0.1611, 0.3057, 0.5594, 0.3513],\n",
            "        [0.6777, 0.9483, 0.4713, 0.1528, 0.6082, 0.8054],\n",
            "        [0.8794, 0.6428, 0.2172, 0.4418, 0.9713, 0.8894]])\n",
            "====================================================================================================\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self,features,labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "  def __getitem__(self,index):\n",
        "    return self.features[index],self.labels[index]"
      ],
      "metadata": {
        "id": "yNY4CD-6Ilrd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaOYZ1bQb2q-",
        "outputId": "f429ee2e-9158-4673-e95a-4f1687b2baa1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugjr2a_qb5Xh",
        "outputId": "00d77918-b3b8-4ac6-e788-2455174a2981"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CustomDataset(X, y)\n",
        "→ Wraps X and y in a dataset object.\n",
        "### DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "- Splits data into batches of 10 for efficient training.\n",
        "- Shuffles data to improve learning.\n",
        "- Makes it easy to iterate during training."
      ],
      "metadata": {
        "id": "4cArlwCkcveO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X,y)\n",
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkaugTQDZ0Tr",
        "outputId": "d054018e-8c23-41a5-9620-a7486dc0704a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7VxYlhcaVim",
        "outputId": "2c4a2903-a194-4361-b0f3-1f91dbf1af48"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.7484, 0.5140, 0.2987, 0.6938, 0.6796, 0.0094]), tensor(1))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "sn2lAZdUaY-1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_features, batch_labels in dataloader:\n",
        "  print(batch_features)\n",
        "  print(batch_labels)\n",
        "  print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TanWBP3idkiS",
        "outputId": "c12ac8c8-948c-4f9e-e9dc-a3abf0e367e2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4035, 0.3907, 0.2688, 0.4353, 0.3443, 0.1205],\n",
            "        [0.4819, 0.2569, 0.9140, 0.5903, 0.9070, 0.9486],\n",
            "        [0.9876, 0.3681, 0.8653, 0.2028, 0.4190, 0.3275],\n",
            "        [0.2205, 0.1346, 0.4050, 0.1876, 0.1370, 0.7164],\n",
            "        [0.6395, 0.3494, 0.4725, 0.3510, 0.5624, 0.8934],\n",
            "        [0.2912, 0.0195, 0.0927, 0.5971, 0.7170, 0.4106],\n",
            "        [0.9285, 0.0211, 0.7063, 0.1076, 0.2905, 0.5838],\n",
            "        [0.4591, 0.4444, 0.3081, 0.6307, 0.4244, 0.4820],\n",
            "        [0.4911, 0.9111, 0.1223, 0.2204, 0.4577, 0.6061],\n",
            "        [0.2491, 0.9403, 0.7755, 0.8972, 0.0646, 0.0385]])\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0])\n",
            "==================================================\n",
            "tensor([[0.2987, 0.5454, 0.4139, 0.1221, 0.5482, 0.0716],\n",
            "        [0.1718, 0.5224, 0.6840, 0.3126, 0.5118, 0.3419],\n",
            "        [0.7473, 0.7798, 0.9900, 0.9377, 0.1537, 0.8897],\n",
            "        [0.8719, 0.3343, 0.4702, 0.2665, 0.4823, 0.5450],\n",
            "        [0.0853, 0.3762, 0.3205, 0.6500, 0.6059, 0.7222],\n",
            "        [0.8815, 0.9248, 0.5555, 0.8305, 0.9029, 0.1655],\n",
            "        [0.7277, 0.2814, 0.5271, 0.9630, 0.1450, 0.5930],\n",
            "        [0.9687, 0.9379, 0.6730, 0.5426, 0.2151, 0.8872],\n",
            "        [0.9449, 0.2446, 0.8405, 0.6939, 0.9687, 0.9183],\n",
            "        [0.7170, 0.1829, 0.4321, 0.2553, 0.8908, 0.8530]])\n",
            "tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
            "==================================================\n",
            "tensor([[0.2158, 0.5957, 0.4134, 0.5058, 0.4373, 0.9655],\n",
            "        [0.4673, 0.8703, 0.2917, 0.5955, 0.0329, 0.4547],\n",
            "        [0.8319, 0.3359, 0.9627, 0.4839, 0.5072, 0.5835],\n",
            "        [0.2238, 0.0486, 0.8420, 0.5740, 0.3979, 0.7728],\n",
            "        [0.6791, 0.9115, 0.3071, 0.5021, 0.3786, 0.6088],\n",
            "        [0.0965, 0.8579, 0.6330, 0.0145, 0.3575, 0.7522],\n",
            "        [0.1991, 0.5905, 0.7555, 0.0317, 0.4007, 0.3050],\n",
            "        [0.3595, 0.4544, 0.4244, 0.8916, 0.1588, 0.5223],\n",
            "        [0.3588, 0.7087, 0.0662, 0.7169, 0.4730, 0.1353],\n",
            "        [0.2038, 0.7420, 0.2721, 0.8693, 0.8531, 0.6995]])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0])\n",
            "==================================================\n",
            "tensor([[0.0771, 0.2302, 0.3039, 0.7645, 0.8320, 0.3998],\n",
            "        [0.3413, 0.2524, 0.7783, 0.1326, 0.7127, 0.0904],\n",
            "        [0.6163, 0.7452, 0.9887, 0.6003, 0.1843, 0.3144],\n",
            "        [0.6478, 0.9683, 0.8222, 0.0620, 0.5100, 0.7791],\n",
            "        [0.5255, 0.0326, 0.8293, 0.3846, 0.7348, 0.3149],\n",
            "        [0.0671, 0.1265, 0.9818, 0.8225, 0.6243, 0.4884],\n",
            "        [0.6883, 0.2423, 0.5956, 0.0339, 0.3082, 0.6250],\n",
            "        [0.2255, 0.6500, 0.7560, 0.5290, 0.5114, 0.5635],\n",
            "        [0.6512, 0.5459, 0.1611, 0.3057, 0.5594, 0.3513],\n",
            "        [0.1812, 0.2039, 0.2554, 0.0667, 0.4029, 0.6956]])\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 1])\n",
            "==================================================\n",
            "tensor([[0.8296, 0.1747, 0.2829, 0.9666, 0.5546, 0.1205],\n",
            "        [0.4770, 0.6874, 0.9437, 0.8137, 0.5849, 0.1876],\n",
            "        [0.3052, 0.7803, 0.5779, 0.2831, 0.3858, 0.4102],\n",
            "        [0.1880, 0.2358, 0.6195, 0.0443, 0.9145, 0.5109],\n",
            "        [0.3776, 0.9760, 0.6006, 0.9888, 0.9478, 0.6795],\n",
            "        [0.5452, 0.2370, 0.4522, 0.9969, 0.5851, 0.3790],\n",
            "        [0.0871, 0.6215, 0.3880, 0.6541, 0.0267, 0.5145],\n",
            "        [0.3033, 0.2507, 0.6972, 0.5489, 0.7244, 0.8616],\n",
            "        [0.2996, 0.0130, 0.8797, 0.0993, 0.0937, 0.7945],\n",
            "        [0.9786, 0.5752, 0.8084, 0.2121, 0.6268, 0.9375]])\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 0])\n",
            "==================================================\n",
            "tensor([[0.8318, 0.3148, 0.9553, 0.1036, 0.9713, 0.3876],\n",
            "        [0.0381, 0.4741, 0.4798, 0.1577, 0.5729, 0.2967],\n",
            "        [0.8794, 0.6428, 0.2172, 0.4418, 0.9713, 0.8894],\n",
            "        [0.3980, 0.4772, 0.4537, 0.9736, 0.7467, 0.0382],\n",
            "        [0.2068, 0.8956, 0.5724, 0.4630, 0.2774, 0.7640],\n",
            "        [0.9474, 0.2379, 0.9606, 0.2483, 0.5563, 0.2781],\n",
            "        [0.4885, 0.6915, 0.4359, 0.6367, 0.6537, 0.0936],\n",
            "        [0.1047, 0.2161, 0.4091, 0.5512, 0.1253, 0.8175],\n",
            "        [0.5332, 0.0549, 0.9872, 0.3369, 0.3187, 0.4036],\n",
            "        [0.4809, 0.3469, 0.9955, 0.3154, 0.9456, 0.5202]])\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 1])\n",
            "==================================================\n",
            "tensor([[0.7667, 0.5106, 0.5484, 0.4717, 0.5271, 0.9290],\n",
            "        [0.2164, 0.9807, 0.7115, 0.7718, 0.3233, 0.3942],\n",
            "        [0.0503, 0.6719, 0.0339, 0.2324, 0.8347, 0.9505],\n",
            "        [0.2663, 0.8363, 0.5950, 0.7301, 0.7248, 0.9349],\n",
            "        [0.6505, 0.5060, 0.9934, 0.5155, 0.9647, 0.8427],\n",
            "        [0.0883, 0.8511, 0.2962, 0.5418, 0.3947, 0.7354],\n",
            "        [0.7566, 0.0880, 0.1550, 0.8078, 0.1599, 0.9444],\n",
            "        [0.3550, 0.5643, 0.2131, 0.1157, 0.8093, 0.3874],\n",
            "        [0.2215, 0.5700, 0.9435, 0.6462, 0.7096, 0.0707],\n",
            "        [0.7484, 0.5140, 0.2987, 0.6938, 0.6796, 0.0094]])\n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 0, 1, 1])\n",
            "==================================================\n",
            "tensor([[0.5008, 0.8301, 0.9907, 0.9875, 0.5571, 0.8058],\n",
            "        [0.2138, 0.7462, 0.0601, 0.7724, 0.4858, 0.2286],\n",
            "        [0.1753, 0.5629, 0.8734, 0.7246, 0.1853, 0.8089],\n",
            "        [0.4397, 0.2831, 0.2467, 0.6330, 0.4300, 0.6828],\n",
            "        [0.3261, 0.2044, 0.5865, 0.7402, 0.3769, 0.9266],\n",
            "        [0.5418, 0.5577, 0.8930, 0.0229, 0.9445, 0.6840],\n",
            "        [0.3919, 0.4411, 0.9767, 0.1356, 0.4588, 0.6754],\n",
            "        [0.2188, 0.3940, 0.7828, 0.0511, 0.7277, 0.7516],\n",
            "        [0.9656, 0.2314, 0.4524, 0.0847, 0.9257, 0.0377],\n",
            "        [0.3591, 0.2662, 0.0539, 0.8810, 0.8171, 0.5528]])\n",
            "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0])\n",
            "==================================================\n",
            "tensor([[0.6690, 0.4359, 0.9203, 0.5703, 0.1306, 0.5975],\n",
            "        [0.1590, 0.4278, 0.9784, 0.6540, 0.1984, 0.0554],\n",
            "        [0.6355, 0.8001, 0.1561, 0.3425, 0.8003, 0.0671],\n",
            "        [0.5776, 0.9000, 0.1609, 0.9306, 0.4972, 0.4618],\n",
            "        [0.2765, 0.9892, 0.9798, 0.6344, 0.7299, 0.9269],\n",
            "        [0.6229, 0.9708, 0.0359, 0.5001, 0.3949, 0.3656],\n",
            "        [0.1383, 0.2346, 0.8305, 0.9651, 0.7634, 0.5301],\n",
            "        [0.9766, 0.4381, 0.4601, 0.5632, 0.7273, 0.1392],\n",
            "        [0.3048, 0.2604, 0.3896, 0.9247, 0.4683, 0.3956],\n",
            "        [0.5507, 0.0065, 0.8948, 0.5024, 0.9782, 0.7293]])\n",
            "tensor([0, 1, 1, 1, 0, 1, 0, 1, 1, 0])\n",
            "==================================================\n",
            "tensor([[0.2111, 0.8961, 0.6753, 0.4878, 0.7563, 0.4630],\n",
            "        [0.4336, 0.8794, 0.7858, 0.6426, 0.8207, 0.3101],\n",
            "        [0.7732, 0.6609, 0.6805, 0.2378, 0.9784, 0.7439],\n",
            "        [0.2823, 0.4437, 0.1454, 0.4515, 0.7684, 0.3523],\n",
            "        [0.4037, 0.8304, 0.3377, 0.1274, 0.5241, 0.6993],\n",
            "        [0.5173, 0.6958, 0.0528, 0.8538, 0.4301, 0.7168],\n",
            "        [0.8930, 0.8396, 0.5133, 0.2139, 0.4861, 0.2790],\n",
            "        [0.6777, 0.9483, 0.4713, 0.1528, 0.6082, 0.8054],\n",
            "        [0.9809, 0.6558, 0.9436, 0.2917, 0.9373, 0.5471],\n",
            "        [0.6282, 0.3804, 0.3477, 0.9349, 0.6239, 0.3063]])\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use these batches during training to prevent **RAM overload** when working with large datasets. By processing **smaller batches**, we efficiently compute gradients and update the model parameters using an optimizer for **each batch**."
      ],
      "metadata": {
        "id": "CrkUHz7-d7pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important Parameters\n",
        "\n",
        "| Parameter     | Purpose                                        | When to Use?                                     |\n",
        "|--------------|--------------------------------|--------------------------------------------------|\n",
        "| `num_workers` | Loads data in parallel using multiple CPU threads | When working with large datasets |\n",
        "| `pin_memory` | Speeds up GPU transfers by using pinned memory | When training on GPU |\n",
        "| `drop_last` | Drops last batch if it’s incomplete | If batch consistency is needed (e.g., BatchNorm) |\n",
        "| `collate_fn` | Customizes how batches are created | When handling variable-sized data |\n",
        "| `sampler` | Controls sample selection | When needing custom sampling (e.g., imbalanced data) |\n",
        "\n"
      ],
      "metadata": {
        "id": "3XMW0h0Vf-uk"
      }
    }
  ]
}
